{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />\n",
    "<img style=\" float:right; display:inline\" src=\"http://opencloud.utsa.edu/wp-content/themes/utsa-oci/images/logo.png\"/>\n",
    "\n",
    "### **University of Texas at San Antonio** \n",
    "<br/>\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 2.5em;\"> **Open Cloud Institute** </span>\n",
    "\n",
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Big Data Analysis with Spark\n",
    "\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Paul Rad, Ph.D.** </span>  \n",
    "\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Nimish Joshi, Research Fellow** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> *Open Cloud Institute, University of Texas at San Antonio, San Antonio, Texas, USA* </span>  \n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> {Nimish.Joshi, Paul.Rad}@utsa.edu </span>  \n",
    "\n",
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Example: Pi.py:** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Let's have a look on example *pi.py*: We first call few libraries as below. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#from random import random\n",
    "#from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> As we are aware of requirement of using SparkContext linrary too, which is also needs to import. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Now, in the main function, we will perform the operation for calulating pi as followed. Here we are using the system argument to calculate the pi value </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"sc = SparkContext(appName = \"PythonPi\")\n",
    "partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2\n",
    "n = 100000 * partitions\n",
    "def(f):\n",
    "    x = random()* 2 - 1\n",
    "    y = random()* 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "count = sc.parallelize(range(1, n+1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "sc.stop\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> combining everything togather into the executable code: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Executable file\n",
    "\n",
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "        Usage: pi [partitions]\n",
    "    \"\"\"\n",
    "    sc = SparkContext(appName=\"PythonPi\")\n",
    "    partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2\n",
    "    n = 100000 * partitions\n",
    "\n",
    "    def f(_):\n",
    "        x = random() * 2 - 1\n",
    "        y = random() * 2 - 1\n",
    "        return 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "    count = sc.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "    print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "\n",
    "    sc.stop()\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!spark-2.0.1-bin-hadoop2.7/./bin/spark-submit /home/cc/spark-2.0.1-bin-hadoop2.7/pi.py 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Example: Wordcount.py:** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Using all the concepts of RDDs we learned in the previous lesson, we will move to implement an example of word count for an existing file inside the spark folder we have called as \"CHANGES.txt\" </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> The main purpose of this progrmam is to calculate the wordcount of the given file, in this case it is text. This would be a good usecase when we are dealing with some of the big chunk file & need some set of things to be calculated. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!cd /home/cc/bin\n",
    "#!pwd\n",
    "#!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# NonExecutable file\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: wordcount <file>\", file=sys.stderr)\n",
    "        exit(-1)\n",
    "    sc = SparkContext(appName=\"PythonWordCount\")\n",
    "    lines = sc.textFile(sys.argv[1], 1)\n",
    "    counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "                  .map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(add)\n",
    "    output = counts.collect()\n",
    "    for (word, count) in output:\n",
    "        print(\"%s: %i\" % (word, count))\n",
    "\n",
    "    sc.stop()\n",
    "\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!spark-2.0.1-bin-hadoop2.7/./bin/spark-submit /home/cc/spark-2.0.1-bin-hadoop2.7/wordcount.py spark-2.0.1-bin-hadoop2.7/CHANGES.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Example(Big Data): Airlines Delay** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> In order to demonstrate a common use of Spark, let's take a look at a common use case where we read in a CSV file of data and compute some aggregate statistic. In this case, we're looking at the on-time flight data set from the U.S. Department of Transportation, recording all U.S. domestic flight departure and arrival times along with their departure and arrival delays for the month of April, 2014. I typically use this data set because one month is manageable for exploration, but the entire data set needs to be computed upon with a cluster. The entire app is as follows: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Spark Application - execute with spark-submit\n",
    "\"\"\"\n",
    "\n",
    "## Imports\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from StringIO import StringIO\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from operator import add, itemgetter\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "## Module Constants\n",
    "APP_NAME = \"Flight Delay Analysis\"\n",
    "DATE_FMT = \"%Y-%m-%d\"\n",
    "TIME_FMT = \"%H%M\"\n",
    "\n",
    "fields   = ('date', 'airline', 'flightnum', 'origin', 'dest', 'dep',\n",
    "            'dep_delay', 'arv', 'arv_delay', 'airtime', 'distance')\n",
    "Flight   = namedtuple('Flight', fields)\n",
    "\n",
    "## Closure Functions\n",
    "def parse(row):\n",
    "    \"\"\"\n",
    "    Parses a row and returns a named tuple.\n",
    "    \"\"\"\n",
    "\n",
    "    row[0]  = datetime.strptime(row[0], DATE_FMT).date()\n",
    "    row[5]  = datetime.strptime(row[5], TIME_FMT).time()\n",
    "    row[6]  = float(row[6])\n",
    "    row[7]  = datetime.strptime(row[7], TIME_FMT).time()\n",
    "    row[8]  = float(row[8])\n",
    "    row[9]  = float(row[9])\n",
    "    row[10] = float(row[10])\n",
    "    return Flight(*row[:11])\n",
    "\n",
    "def split(line):\n",
    "    \"\"\"\n",
    "    Operator function for splitting a line with csv module\n",
    "    \"\"\"\n",
    "    reader = csv.reader(StringIO(line))\n",
    "    return reader.next()\n",
    "\n",
    "def plot(delays):\n",
    "    \"\"\"\n",
    "    Show a bar chart of the total delay per airline\n",
    "    \"\"\"\n",
    "    airlines = [d[0] for d in delays]\n",
    "    minutes  = [d[1] for d in delays]\n",
    "    index    = list(xrange(len(airlines)))\n",
    "\n",
    "    fig, axe = plt.subplots()\n",
    "    bars = axe.barh(index, minutes)\n",
    "\n",
    "    # Add the total minutes to the right\n",
    "    for idx, air, min in zip(index, airlines, minutes):\n",
    "        if min > 0:\n",
    "            bars[idx].set_color('#d9230f')\n",
    "            axe.annotate(\" %0.0f min\" % min, xy=(min+1, idx+0.5), va='center')\n",
    "        else:\n",
    "            bars[idx].set_color('#469408')\n",
    "            axe.annotate(\" %0.0f min\" % min, xy=(10, idx+0.5), va='center')\n",
    "\n",
    "    # Set the ticks\n",
    "    ticks = plt.yticks([idx+ 0.5 for idx in index], airlines)\n",
    "    xt = plt.xticks()[0]\n",
    "    plt.xticks(xt, [' '] * len(xt))\n",
    "\n",
    "    # minimize chart junk\n",
    "    plt.grid(axis = 'x', color ='white', linestyle='-')\n",
    "\n",
    "    plt.title('Total Minutes Delayed per Airline')\n",
    "    save_name = 'home/cc/spark-2.0.1-bin-hadoop2.7/image.png'\n",
    "    plt.savefig(save_name)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "## Main functionality\n",
    "def main(sc):\n",
    "\n",
    "    # Load the airlines lookup dictionary\n",
    "    airlines = dict(sc.textFile(\"airlines.csv\").map(split).collect())\n",
    "\n",
    "    # Broadcast the lookup dictionary to the cluster\n",
    "    airline_lookup = sc.broadcast(airlines)\n",
    "\n",
    "    # Read the CSV Data into an RDD\n",
    "    flights = sc.textFile(\"flights.csv\").map(split).map(parse)\n",
    "\n",
    "    # Map the total delay to the airline (joined using the broadcast value)\n",
    "    delays  = flights.map(lambda f: (airline_lookup.value[f.airline],\n",
    "                                     add(f.dep_delay, f.arv_delay)))\n",
    "\n",
    "    # Reduce the total delay for the month to the airline\n",
    "    delays  = delays.reduceByKey(add).collect()\n",
    "    delays  = sorted(delays, key=itemgetter(1))\n",
    "\n",
    "    # Provide output from the driver\n",
    "    for d in delays:\n",
    "        print \"%0.0f minutes delayed\\t%s\" % (d[1], d[0])\n",
    "\n",
    "    # Show a bar chart of the delays\n",
    "    plot(delays)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure Spark\n",
    "    conf = SparkConf().setMaster(\"local[*]\")\n",
    "    conf = conf.setAppName(APP_NAME)\n",
    "    sc   = SparkContext(conf=conf)\n",
    "\n",
    "    # Execute Main functionality\n",
    "    main(sc)\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> So what is this code doing?\n",
    "Let's look particularly at the main function which does the work most directly related to Spark. First, we load up a **CSV file** into an RDD, then map the split function to it. The split function parses each line of text using the csv module and returns a tuple that represents the row. Finally we pass the collect action to the RDD, which brings the data from the RDD back to the driver as a Python list. In this case, **airlines.csv** is a small jump table that will allow us to join airline codes with the airline full name. We will store this jump table as a Python dictionary and then broadcast it to the node  sc.broadcast, which also works for the case when we have a cluster of Spark. \n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Next, the main function loads the much larger **flights.csv**. After splitting the CSV rows, we map the parse function to the CSV row, which converts dates and times to Python dates and times, and casts floating point numbers appropriately. It also stores the row as a NamedTuple called Flight for efficient ease of use. </span>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\">With an RDD of Flight objects in hand, we map an anonymous function that transforms the RDD to a series of key-value pairs where the key is the name of the airline and the value is the sum of the arrival and departure delays. Each airline has its delay summed together using the reduceByKey action and the add operator, and this RDD is collected back to the driver (again the number airlines in the data is relatively small). Finally the delays are sorted in ascending order, then the output is printed to the console as well as visualized using matplotlib. </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> To run this code, use the spark-submit command as follows:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!spark-2.0.1-bin-hadoop2.7/./bin/spark-submit /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> This will generathe the below figure: </span>\n",
    "<div style=\"width:830; background-color:white; height:540px; overflow:scroll; overflow-x: scroll;overflow-y: hidden;\">\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"\" width=\"160\" height=\"70\"/>\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"http://129.114.111.241:8888/tree/spark-2.0.1-bin-hadoop2.7/delays.png\" width=\"860\" height=\"420\"/> </div>\n",
    "\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\">This will create a Spark job using the localhost as the master, and look for the two **CSV** files in an ontime directory that is in the same directory as the python code is in. The final result shows that the total delays (in minutes) for the perticlar month. on our case we took the data for the month of April that goes from arriving early if you're flying out of the continental U.S. to Hawaii or Alaska to an aggregate total delay for most big airlines. Note especially that we can visualize the result using matplotlib directly on the driver program, with the above python code: </span>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
