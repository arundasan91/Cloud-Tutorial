{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />\n",
    "<img style=\" float:right; display:inline\" src=\"http://opencloud.utsa.edu/wp-content/themes/utsa-oci/images/logo.png\"/>\n",
    "\n",
    "### **University of Texas at San Antonio** \n",
    "<br/>\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 2.5em;\"> **Open Cloud Institute** </span>\n",
    "\n",
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Big Data Analysis with Spark\n",
    "\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Paul Rad, Ph.D.** </span>  \n",
    "\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Nimish Joshi, Research Fellow** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> *Open Cloud Institute, University of Texas at San Antonio, San Antonio, Texas, USA* </span>  \n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> {Nimish.Joshi, Paul.Rad}@utsa.edu </span>  \n",
    "\n",
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Example: Pi.py:** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Let's have a look on example *pi.py*: We first call few libraries as below. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#from random import random\n",
    "#from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> As we are aware of requirement of using SparkContext linrary too, which is also needs to import. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Now, in the main function, we will perform the operation for calulating pi as followed. Here we are using the system argument to calculate the pi value </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"sc = SparkContext(appName = \"PythonPi\")\n",
    "partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2\n",
    "n = 100000 * partitions\n",
    "def(f):\n",
    "    x = random()* 2 - 1\n",
    "    y = random()* 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "count = sc.parallelize(range(1, n+1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "sc.stop\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> combining everything togather into the executable code: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Executable file\n",
    "\n",
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "        Usage: pi [partitions]\n",
    "    \"\"\"\n",
    "    sc = SparkContext(appName=\"PythonPi\")\n",
    "    partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2\n",
    "    n = 100000 * partitions\n",
    "\n",
    "    def f(_):\n",
    "        x = random() * 2 - 1\n",
    "        y = random() * 2 - 1\n",
    "        return 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "    count = sc.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "    print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "\n",
    "    sc.stop()\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!spark-2.0.1-bin-hadoop2.7/./bin/spark-submit /home/cc/spark-2.0.1-bin-hadoop2.7/pi.py 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Example: Wordcount.py:** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Using all the concepts of RDDs we learned in the previous lesson, we will move to implement an example of word count for an existing file inside the spark folder we have called as \"CHANGES.txt\" </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> The main purpose of this progrmam is to calculate the wordcount of the given file, in this case it is text. This would be a good usecase when we are dealing with some of the big chunk file & need some set of things to be calculated. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!cd /home/cc/bin\n",
    "#!pwd\n",
    "#!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# NonExecutable file\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: wordcount <file>\", file=sys.stderr)\n",
    "        exit(-1)\n",
    "    sc = SparkContext(appName=\"PythonWordCount\")\n",
    "    lines = sc.textFile(sys.argv[1], 1)\n",
    "    counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "                  .map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(add)\n",
    "    output = counts.collect()\n",
    "    for (word, count) in output:\n",
    "        print(\"%s: %i\" % (word, count))\n",
    "\n",
    "    sc.stop()\n",
    "\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!spark-2.0.1-bin-hadoop2.7/./bin/spark-submit /home/cc/spark-2.0.1-bin-hadoop2.7/wordcount.py spark-2.0.1-bin-hadoop2.7/CHANGES.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Example(Big Data): Airline Delay** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> In order to demonstrate a common use of Spark, let's take a look at a common use case where we read in a CSV file of data and compute some aggregate statistic. In this case, we're looking at the on-time flight data set from the U.S. Department of Transportation, recording all U.S. domestic flight departure and arrival times along with their departure and arrival delays for the month of April, 2014. I typically use this data set because one month is manageable for exploration, but the entire data set needs to be computed upon with a cluster. The entire app is as follows: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Spark Application - execute with spark-submit\n",
    "\"\"\"\n",
    "\n",
    "## Imports\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from StringIO import StringIO\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from operator import add, itemgetter\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "## Module Constants\n",
    "APP_NAME = \"Flight Delay Analysis\"\n",
    "DATE_FMT = \"%Y-%m-%d\"\n",
    "TIME_FMT = \"%H%M\"\n",
    "\n",
    "fields   = ('date', 'airline', 'flightnum', 'origin', 'dest', 'dep',\n",
    "            'dep_delay', 'arv', 'arv_delay', 'airtime', 'distance')\n",
    "Flight   = namedtuple('Flight', fields)\n",
    "\n",
    "## Closure Functions\n",
    "def parse(row):\n",
    "    \"\"\"\n",
    "    Parses a row and returns a named tuple.\n",
    "    \"\"\"\n",
    "\n",
    "    row[0]  = datetime.strptime(row[0], DATE_FMT).date()\n",
    "    row[5]  = datetime.strptime(row[5], TIME_FMT).time()\n",
    "    row[6]  = float(row[6])\n",
    "    row[7]  = datetime.strptime(row[7], TIME_FMT).time()\n",
    "    row[8]  = float(row[8])\n",
    "    row[9]  = float(row[9])\n",
    "    row[10] = float(row[10])\n",
    "    return Flight(*row[:11])\n",
    "\n",
    "def split(line):\n",
    "    \"\"\"\n",
    "    Operator function for splitting a line with csv module\n",
    "    \"\"\"\n",
    "    reader = csv.reader(StringIO(line))\n",
    "    return reader.next()\n",
    "\n",
    "def plot(delays):\n",
    "    \"\"\"\n",
    "    Show a bar chart of the total delay per airline\n",
    "    \"\"\"\n",
    "    airlines = [d[0] for d in delays]\n",
    "    minutes  = [d[1] for d in delays]\n",
    "    index    = list(xrange(len(airlines)))\n",
    "\n",
    "    fig, axe = plt.subplots()\n",
    "    bars = axe.barh(index, minutes)\n",
    "\n",
    "    # Add the total minutes to the right\n",
    "    for idx, air, min in zip(index, airlines, minutes):\n",
    "        if min > 0:\n",
    "            bars[idx].set_color('#d9230f')\n",
    "            axe.annotate(\" %0.0f min\" % min, xy=(min+1, idx+0.5), va='center')\n",
    "        else:\n",
    "            bars[idx].set_color('#469408')\n",
    "            axe.annotate(\" %0.0f min\" % min, xy=(10, idx+0.5), va='center')\n",
    "\n",
    "    # Set the ticks\n",
    "    ticks = plt.yticks([idx+ 0.5 for idx in index], airlines)\n",
    "    xt = plt.xticks()[0]\n",
    "    plt.xticks(xt, [' '] * len(xt))\n",
    "\n",
    "    # minimize chart junk\n",
    "    plt.grid(axis = 'x', color ='white', linestyle='-')\n",
    "\n",
    "    plt.title('Total Minutes Delayed per Airline')\n",
    "    save_name = 'home/cc/spark-2.0.1-bin-hadoop2.7/image.png'\n",
    "    plt.savefig(save_name)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "## Main functionality\n",
    "def main(sc):\n",
    "\n",
    "    # Load the airlines lookup dictionary\n",
    "    airlines = dict(sc.textFile(\"airlines.csv\").map(split).collect())\n",
    "\n",
    "    # Broadcast the lookup dictionary to the cluster\n",
    "    airline_lookup = sc.broadcast(airlines)\n",
    "\n",
    "    # Read the CSV Data into an RDD\n",
    "    flights = sc.textFile(\"flights.csv\").map(split).map(parse)\n",
    "\n",
    "    # Map the total delay to the airline (joined using the broadcast value)\n",
    "    delays  = flights.map(lambda f: (airline_lookup.value[f.airline],\n",
    "                                     add(f.dep_delay, f.arv_delay)))\n",
    "\n",
    "    # Reduce the total delay for the month to the airline\n",
    "    delays  = delays.reduceByKey(add).collect()\n",
    "    delays  = sorted(delays, key=itemgetter(1))\n",
    "\n",
    "    # Provide output from the driver\n",
    "    for d in delays:\n",
    "        print \"%0.0f minutes delayed\\t%s\" % (d[1], d[0])\n",
    "\n",
    "    # Show a bar chart of the delays\n",
    "    plot(delays)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure Spark\n",
    "    conf = SparkConf().setMaster(\"local[*]\")\n",
    "    conf = conf.setAppName(APP_NAME)\n",
    "    sc   = SparkContext(conf=conf)\n",
    "\n",
    "    # Execute Main functionality\n",
    "    main(sc)\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> So what is this code doing?\n",
    "Let's look particularly at the main function which does the work most directly related to Spark. First, we load up a **CSV file** into an RDD, then map the split function to it. The split function parses each line of text using the csv module and returns a tuple that represents the row. Finally we pass the collect action to the RDD, which brings the data from the RDD back to the driver as a Python list. In this case, **airlines.csv** is a small jump table that will allow us to join airline codes with the airline full name. We will store this jump table as a Python dictionary and then broadcast it to the node  sc.broadcast, which also works for the case when we have a cluster of Spark. \n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Next, the main function loads the much larger **flights.csv**. After splitting the CSV rows, we map the parse function to the CSV row, which converts dates and times to Python dates and times, and casts floating point numbers appropriately. It also stores the row as a NamedTuple called Flight for efficient ease of use. </span>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\">With an RDD of Flight objects in hand, we map an anonymous function that transforms the RDD to a series of key-value pairs where the key is the name of the airline and the value is the sum of the arrival and departure delays. Each airline has its delay summed together using the reduceByKey action and the add operator, and this RDD is collected back to the driver (again the number airlines in the data is relatively small). Finally the delays are sorted in ascending order, then the output is printed to the console as well as visualized using matplotlib. </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> To run this code, use the spark-submit command as follows:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "16/11/06 06:54:51 INFO SparkContext: Running Spark version 2.0.1\n",
      "16/11/06 06:54:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/11/06 06:54:51 WARN Utils: Your hostname, nimish-spark2 resolves to a loopback address: 127.0.0.1; using 192.168.0.206 instead (on interface eth0)\n",
      "16/11/06 06:54:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "16/11/06 06:54:51 INFO SecurityManager: Changing view acls to: root\n",
      "16/11/06 06:54:51 INFO SecurityManager: Changing modify acls to: root\n",
      "16/11/06 06:54:51 INFO SecurityManager: Changing view acls groups to: \n",
      "16/11/06 06:54:51 INFO SecurityManager: Changing modify acls groups to: \n",
      "16/11/06 06:54:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "16/11/06 06:54:52 INFO Utils: Successfully started service 'sparkDriver' on port 40348.\n",
      "16/11/06 06:54:52 INFO SparkEnv: Registering MapOutputTracker\n",
      "16/11/06 06:54:52 INFO SparkEnv: Registering BlockManagerMaster\n",
      "16/11/06 06:54:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-581677e9-ac23-464a-bd06-9f44bc29e3a7\n",
      "16/11/06 06:54:52 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "16/11/06 06:54:52 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "16/11/06 06:54:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "16/11/06 06:54:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.206:4040\n",
      "16/11/06 06:54:52 INFO SparkContext: Added file file:/home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py at file:/home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py with timestamp 1478415292566\n",
      "16/11/06 06:54:52 INFO Utils: Copying /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py to /tmp/spark-aa6dc0d2-fe6b-4b0f-8781-1498335c1867/userFiles-61240c06-3ad8-4c01-b70a-09a7977c5f12/flightdelay.py\n",
      "16/11/06 06:54:52 INFO Executor: Starting executor ID driver on host localhost\n",
      "16/11/06 06:54:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58346.\n",
      "16/11/06 06:54:52 INFO NettyBlockTransferService: Server created on 192.168.0.206:58346\n",
      "16/11/06 06:54:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.206, 58346)\n",
      "16/11/06 06:54:52 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.206:58346 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.206, 58346)\n",
      "16/11/06 06:54:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.206, 58346)\n",
      "16/11/06 06:54:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 193.9 KB, free 366.1 MB)\n",
      "16/11/06 06:54:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.9 KB, free 366.1 MB)\n",
      "16/11/06 06:54:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.206:58346 (size: 22.9 KB, free: 366.3 MB)\n",
      "16/11/06 06:54:53 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2\n",
      "16/11/06 06:54:53 INFO FileInputFormat: Total input paths to process : 1\n",
      "16/11/06 06:54:53 INFO SparkContext: Starting job: collect at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:82\n",
      "16/11/06 06:54:53 INFO DAGScheduler: Got job 0 (collect at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:82) with 2 output partitions\n",
      "16/11/06 06:54:53 INFO DAGScheduler: Final stage: ResultStage 0 (collect at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:82)\n",
      "16/11/06 06:54:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "16/11/06 06:54:53 INFO DAGScheduler: Missing parents: List()\n",
      "16/11/06 06:54:53 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at collect at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:82), which has no missing parents\n",
      "16/11/06 06:54:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.6 KB, free 366.1 MB)\n",
      "16/11/06 06:54:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.4 KB, free 366.1 MB)\n",
      "16/11/06 06:54:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.206:58346 (size: 3.4 KB, free: 366.3 MB)\n",
      "16/11/06 06:54:53 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1012\n",
      "16/11/06 06:54:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at collect at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:82)\n",
      "16/11/06 06:54:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n",
      "16/11/06 06:54:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5579 bytes)\n",
      "16/11/06 06:54:53 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5579 bytes)\n",
      "16/11/06 06:54:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "16/11/06 06:54:53 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "16/11/06 06:54:53 INFO Executor: Fetching file:/home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py with timestamp 1478415292566\n",
      "16/11/06 06:54:53 INFO Utils: /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py has been previously copied to /tmp/spark-aa6dc0d2-fe6b-4b0f-8781-1498335c1867/userFiles-61240c06-3ad8-4c01-b70a-09a7977c5f12/flightdelay.py\n",
      "16/11/06 06:54:53 INFO HadoopRDD: Input split: file:/home/cc/spark-2.0.1-bin-hadoop2.7/airlines.csv:29054+29054\n",
      "16/11/06 06:54:53 INFO HadoopRDD: Input split: file:/home/cc/spark-2.0.1-bin-hadoop2.7/airlines.csv:0+29054\n",
      "16/11/06 06:54:53 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/11/06 06:54:53 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/11/06 06:54:53 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/11/06 06:54:53 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/11/06 06:54:53 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/11/06 06:54:53 INFO PythonRunner: Times: total = 264, boot = 212, init = 43, finish = 9\n",
      "16/11/06 06:54:53 INFO PythonRunner: Times: total = 267, boot = 209, init = 48, finish = 10\n",
      "16/11/06 06:54:53 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 39256 bytes result sent to driver\n",
      "16/11/06 06:54:53 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 39227 bytes result sent to driver\n",
      "16/11/06 06:54:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 412 ms on localhost (1/2)\n",
      "16/11/06 06:54:53 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 385 ms on localhost (2/2)\n",
      "16/11/06 06:54:53 INFO DAGScheduler: ResultStage 0 (collect at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:82) finished in 0.436 s\n",
      "16/11/06 06:54:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "16/11/06 06:54:53 INFO DAGScheduler: Job 0 finished: collect at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:82, took 0.546259 s\n",
      "16/11/06 06:54:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 296.0 B, free 366.1 MB)\n",
      "16/11/06 06:54:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KB, free 366.0 MB)\n",
      "16/11/06 06:54:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.206:58346 (size: 34.1 KB, free: 366.2 MB)\n",
      "16/11/06 06:54:53 INFO SparkContext: Created broadcast 2 from broadcast at PythonRDD.scala:482\n",
      "16/11/06 06:54:54 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 193.9 KB, free 365.9 MB)\n",
      "16/11/06 06:54:54 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.9 KB, free 365.8 MB)\n",
      "16/11/06 06:54:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.206:58346 (size: 22.9 KB, free: 366.2 MB)\n",
      "16/11/06 06:54:54 INFO SparkContext: Created broadcast 3 from textFile at NativeMethodAccessorImpl.java:-2\n",
      "16/11/06 06:54:54 INFO FileInputFormat: Total input paths to process : 1\n",
      "16/11/06 06:54:54 INFO SparkContext: Starting job: collect at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:95\n",
      "16/11/06 06:54:54 INFO DAGScheduler: Registering RDD 6 (reduceByKey at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:95)\n",
      "16/11/06 06:54:54 INFO DAGScheduler: Got job 1 (collect at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:95) with 2 output partitions\n",
      "16/11/06 06:54:54 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:95)\n",
      "16/11/06 06:54:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "16/11/06 06:54:54 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
      "16/11/06 06:54:54 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[6] at reduceByKey at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:95), which has no missing parents\n",
      "16/11/06 06:54:54 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 10.6 KB, free 365.8 MB)\n",
      "16/11/06 06:54:54 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.6 KB, free 365.8 MB)\n",
      "16/11/06 06:54:54 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.0.206:58346 (size: 6.6 KB, free: 366.2 MB)\n",
      "16/11/06 06:54:54 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1012\n",
      "16/11/06 06:54:54 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[6] at reduceByKey at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:95)\n",
      "16/11/06 06:54:54 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks\n",
      "16/11/06 06:54:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5567 bytes)\n",
      "16/11/06 06:54:54 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, PROCESS_LOCAL, 5567 bytes)\n",
      "16/11/06 06:54:54 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
      "16/11/06 06:54:54 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\n",
      "16/11/06 06:54:54 INFO HadoopRDD: Input split: file:/home/cc/spark-2.0.1-bin-hadoop2.7/flights.csv:15829953+15829953\n",
      "16/11/06 06:54:54 INFO HadoopRDD: Input split: file:/home/cc/spark-2.0.1-bin-hadoop2.7/flights.csv:0+15829953\n",
      "16/11/06 06:54:54 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.206:58346 in memory (size: 3.4 KB, free: 366.2 MB)\n",
      "16/11/06 06:55:03 INFO PythonRunner: Times: total = 8830, boot = -342, init = 380, finish = 8792\n",
      "16/11/06 06:55:03 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 1933 bytes result sent to driver\n",
      "16/11/06 06:55:03 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 9020 ms on localhost (1/2)\n",
      "16/11/06 06:55:03 INFO PythonRunner: Times: total = 8993, boot = -353, init = 393, finish = 8953\n",
      "16/11/06 06:55:03 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1767 bytes result sent to driver\n",
      "16/11/06 06:55:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 9046 ms on localhost (2/2)\n",
      "16/11/06 06:55:03 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "16/11/06 06:55:03 INFO DAGScheduler: ShuffleMapStage 1 (reduceByKey at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:95) finished in 9.049 s\n",
      "16/11/06 06:55:03 INFO DAGScheduler: looking for newly runnable stages\n",
      "16/11/06 06:55:03 INFO DAGScheduler: running: Set()\n",
      "16/11/06 06:55:03 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
      "16/11/06 06:55:03 INFO DAGScheduler: failed: Set()\n",
      "16/11/06 06:55:03 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[9] at collect at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:95), which has no missing parents\n",
      "16/11/06 06:55:03 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 6.1 KB, free 365.8 MB)\n",
      "16/11/06 06:55:03 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.7 KB, free 365.8 MB)\n",
      "16/11/06 06:55:03 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.0.206:58346 (size: 3.7 KB, free: 366.2 MB)\n",
      "16/11/06 06:55:03 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1012\n",
      "16/11/06 06:55:03 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[9] at collect at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:95)\n",
      "16/11/06 06:55:03 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks\n",
      "16/11/06 06:55:03 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, localhost, partition 0, ANY, 5329 bytes)\n",
      "16/11/06 06:55:03 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, localhost, partition 1, ANY, 5329 bytes)\n",
      "16/11/06 06:55:03 INFO Executor: Running task 1.0 in stage 2.0 (TID 5)\n",
      "16/11/06 06:55:03 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)\n",
      "16/11/06 06:55:03 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks\n",
      "16/11/06 06:55:03 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks\n",
      "16/11/06 06:55:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
      "16/11/06 06:55:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
      "16/11/06 06:55:03 INFO PythonRunner: Times: total = 4, boot = -76, init = 80, finish = 0\n",
      "16/11/06 06:55:03 INFO Executor: Finished task 1.0 in stage 2.0 (TID 5). 2241 bytes result sent to driver\n",
      "16/11/06 06:55:03 INFO PythonRunner: Times: total = 5, boot = -242, init = 247, finish = 0\n",
      "16/11/06 06:55:03 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 49 ms on localhost (1/2)\n",
      "16/11/06 06:55:03 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 2152 bytes result sent to driver\n",
      "16/11/06 06:55:03 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 55 ms on localhost (2/2)\n",
      "16/11/06 06:55:03 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "16/11/06 06:55:03 INFO DAGScheduler: ResultStage 2 (collect at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:95) finished in 0.056 s\n",
      "16/11/06 06:55:03 INFO DAGScheduler: Job 1 finished: collect at /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py:95, took 9.188121 s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "-45442 minutes delayed\tAlaska Airlines Inc.: AS \n",
      " \n",
      "-20654 minutes delayed\tHawaiian Airlines Inc.: HA \n",
      " \n",
      "39247 minutes delayed\tAirTran Airways Corporation: FL \n",
      " \n",
      "40841 minutes delayed\tVirgin America: VX \n",
      " \n",
      "108480 minutes delayed\tFrontier Airlines Inc.: F9 \n",
      " \n",
      "177717 minutes delayed\tUS Airways Inc.: US \n",
      " \n",
      "279981 minutes delayed\tJetBlue Airways: B6 \n",
      " \n",
      "390614 minutes delayed\tUnited Air Lines Inc.: UA \n",
      " \n",
      "431755 minutes delayed\tAmerican Airlines Inc.: AA \n",
      " \n",
      "461753 minutes delayed\tDelta Air Lines Inc.: DL \n",
      " \n",
      "493527 minutes delayed\tEnvoy Air: MQ \n",
      " \n",
      "519867 minutes delayed\tSkyWest Airlines Inc.: OO \n",
      " \n",
      "1160058 minutes delayed\tExpressJet Airlines Inc.: EV \n",
      " \n",
      "2181955 minutes delayed\tSouthwest Airlines Co.: WN \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py\", line 114, in <module>\n",
      "    main(sc)\n",
      "  File \"/home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py\", line 105, in main\n",
      "    plot(delays)\n",
      "  File \"/home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py\", line 52, in plot\n",
      "    fig, axe = plt.subplots()\n",
      "  File \"/usr/lib/pymodules/python2.7/matplotlib/pyplot.py\", line 1046, in subplots\n",
      "    fig = figure(**fig_kw)\n",
      "  File \"/usr/lib/pymodules/python2.7/matplotlib/pyplot.py\", line 423, in figure\n",
      "    **kwargs)\n",
      "  File \"/usr/lib/pymodules/python2.7/matplotlib/backends/backend_tkagg.py\", line 79, in new_figure_manager\n",
      "    return new_figure_manager_given_figure(num, figure)\n",
      "  File \"/usr/lib/pymodules/python2.7/matplotlib/backends/backend_tkagg.py\", line 87, in new_figure_manager_given_figure\n",
      "    window = Tk.Tk()\n",
      "  File \"/usr/lib/python2.7/lib-tk/Tkinter.py\", line 1767, in __init__\n",
      "    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)\n",
      "_tkinter.TclError: no display name and no $DISPLAY environment variable\n",
      "16/11/06 06:55:03 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "16/11/06 06:55:03 INFO SparkUI: Stopped Spark web UI at http://192.168.0.206:4040\n",
      "16/11/06 06:55:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "16/11/06 06:55:03 INFO MemoryStore: MemoryStore cleared\n",
      "16/11/06 06:55:03 INFO BlockManager: BlockManager stopped\n",
      "16/11/06 06:55:03 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "16/11/06 06:55:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "16/11/06 06:55:03 INFO SparkContext: Successfully stopped SparkContext\n",
      "16/11/06 06:55:03 INFO ShutdownHookManager: Shutdown hook called\n",
      "16/11/06 06:55:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-aa6dc0d2-fe6b-4b0f-8781-1498335c1867\n",
      "16/11/06 06:55:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-aa6dc0d2-fe6b-4b0f-8781-1498335c1867/pyspark-09683de5-8240-414c-86ad-73d438eb4e3a\n"
     ]
    }
   ],
   "source": [
    "!spark-2.0.1-bin-hadoop2.7/./bin/spark-submit /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> This will generathe the below figure: </span>\n",
    "<div style=\"width:830; background-color:white; height:540px; overflow:scroll; overflow-x: scroll;overflow-y: hidden;\">\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"\" width=\"160\" height=\"70\"/>\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"http://129.114.111.241:8888/tree/spark-2.0.1-bin-hadoop2.7/delays.png\" width=\"860\" height=\"420\"/> </div>\n",
    "\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\">This will create a Spark job using the localhost as the master, and look for the two **CSV** files in an ontime directory that is in the same directory as the python code is in. The final result shows that the total delays (in minutes) for the perticlar month. on our case we took the data for the month of April that goes from arriving early if you're flying out of the continental U.S. to Hawaii or Alaska to an aggregate total delay for most big airlines. Note especially that we can visualize the result using matplotlib directly on the driver program, with the above python code: </span>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
